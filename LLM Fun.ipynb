{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM + Book Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.prompts import ChatPromptTemplate # Prompt templates convert raw user input to better input to the LLM (guide reponse)\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_core.output_parsers import StrOutputParser # original model output is a message but this function parses it to a string (easier to work with)\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain_community.embeddings import OllamaEmbeddings # converts documents into vector chunks\n",
    "from langchain_community.vectorstores import FAISS # simple local vector databse (vectorstore) in place of pinecone/chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain #This chain will take an incoming question, look up relevant documents, then pass those documents along with the original question into an LLM and \n",
    "from langchain.chains import create_retrieval_chain, create_history_aware_retriever                     #ask it to answer the original question.\n",
    "from langchain_community.vectorstores import Chroma # Vector database we'll use \n",
    "from langchain_core.messages import HumanMessage, AIMessage # Used to frame history to LLM and retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"llama2:13b\" # mistral, llama2, kdl_copilot_llama3, llama3, llama2:13b\n",
    "llm = Ollama(model=model)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Your are a librarian who provides relavent book reccomendations\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "output1 = chain.invoke({\"input\": \"Find me some interesting general fiction books to read\"}) \n",
    "\n",
    "print(output1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example prompts \n",
    "\n",
    "str1 = \"\"\"Ah, a lover of mystery books, are you? Well, you've come to the right place! As a librarian, I have access to a vast array of mysterious tales that will keep you guessing until the very end. Here are some of my top recommendations:\n",
    "\n",
    "1. \"And Then There Were None\" by Agatha Christie - This classic whodunit follows ten strangers who are invited to an isolated island, only to be killed off one by one. It's a timeless tale of murder and deception that will keep you on the edge of your seat.\n",
    "2. \"The Girl with the Dragon Tattoo\" by Stieg Larsson - This bestselling novel introduces us to Lisbeth Salander, a brilliant but troubled hacker, and journalist Mikael Blomkvist, who team up to solve a decades-old mystery. It's a gripping thriller with plenty of twists and turns.\n",
    "3. \"Gone Girl\" by Gillian Flynn - This psychological thriller follows Nick and Amy Dunne, a married couple whose seemingly perfect life turns out to be a facade. When Amy goes missing, Nick becomes the prime suspect, but nothing is as it seems in this twisty tale.\n",
    "4. \"The Lincoln Rhyme\" series by Jeffery Deaver - These books follow a quadriplegic detective and his partner as they investigate crimes in New York City. With a unique protagonist and a focus on forensic science, these mysteries are both informative and engaging.\n",
    "5. \"The Cuckoo's Calling\" by Robert Galbraith (J.K. Rowling) - When a troubled model falls to her death, private investigator Cormoran Strike is hired to investigate. This book has all the hallmarks of a classic mystery, with a complex plot and a cast of suspicious characters.\n",
    "\n",
    "I hope these recommendations have piqued your interest! Is there anything specific you're looking for in a mystery book? Perhaps a particular theme or setting? Let me know and I can suggest some more tailored recommendations. Happy reading!\"\"\"\n",
    "\n",
    "cor_str1 = {\n",
    "\"And Then There Were None\": \"Agatha Christie\",\n",
    "\"The Girl with the Dragon Tattoo\": \"Stieg Larsson\",\n",
    "\"Gone Girl\": \"Gillian Flynn\",\n",
    "\"The Lincoln Rhyme series\": \"Jeffery Deaver\",\n",
    "\"The Cuckoo's Calling\": \"Robert Galbraith (J.K. Rowling)\"\n",
    "}\n",
    "\n",
    "str2 = \"\"\"Well hello there! *adjusting glasses* As a librarian, I'd be happy to help you find some interesting general fiction books to read. *smiling*\n",
    "\n",
    "Let me see... *pausing for a moment of thought* If you're looking for something with a mix of humor and heartwarming moments, I recommend \"Where'd You Go, Bernadette\" by Maria Semple. It's a delightful read about a dysfunctional family and their adventures. *smiling*\n",
    "\n",
    "If you're in the mood for something more introspective and poignant, I suggest \"The Guernsey Literary and Potato Peel Pie Society\" by Mary Ann Shaffer and Annie Barrows. It's a beautifully written epistolary novel set in post-WWII England, exploring themes of love, loss, and the power of literature to bring people together. *nodding*\n",
    "\n",
    "For something a bit more magical and whimsical, you might enjoy \"The Particular Sadness of Lemon Cake\" by Aimee Bender. It's a captivating tale about a young girl who can taste people's emotions through the food they prepare, and it's full of warmth and wonder. *smiling*\n",
    "\n",
    "Lastly, if you're looking for a page-turner with plenty of twists and turns, I recommend \"The Girl on the Train\" by Paula Hawkins. It's a psychological thriller about a woman who becomes obsessed with a perfect couple she watches on her daily commute, and it's full of surprises and suspense. *raising an eyebrow*\n",
    "\n",
    "I hope these recommendations inspire you to find your next great read! Do you have any other preferences or genres you'd like to explore? * leaning forward with interest*\"\"\"\n",
    "\n",
    "cor_str2 = {\n",
    "\"Where'd You Go, Bernadette\": \"Maria Semple\",\n",
    "\"The Guernsey Literary and Potato Peel Pie Society\": \"Mary Ann Shaffer and Annie Barrows\",\n",
    "\"The Particular Sadness of Lemon Cake\": \"Aimee Bender\",\n",
    "\"The Girl on the Train\": \"Paula Hawkins\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2 = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"Extract the author and title from the following output: {str1} in the format of a python dictionary\"),\n",
    "        (\"ai\", \"{cor_str1}\"),\n",
    "\n",
    "        (\"human\", \"Extract the author and title from the following output: {str2} in the format of a python dictionary\"),\n",
    "        (\"ai\", \"{cor_str2}\"),\n",
    "\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain2 = prompt2 | llm | output_parser\n",
    "\n",
    "output2 = chain2.invoke({\"input\": \"Extract the only the author and title from the following output {output1} \"}) \n",
    "print(output2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM + Email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import requests\n",
    "import pandas as pd\n",
    "import win32com.client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Formating then sending an outlook email\n",
    "\n",
    "ol=win32com.client.Dispatch(\"outlook.application\")\n",
    "\n",
    "olmailitem=0x0 #size of the new email\n",
    "newmail=ol.CreateItem(olmailitem)\n",
    "\n",
    "newmail.Subject= 'Testing Mail'\n",
    "newmail.To='ryanike11@icloud.com'\n",
    "#newmail.CC='xyz@example.com'\n",
    "\n",
    "newmail.Body= 'Hello, this is a test email.'\n",
    "\n",
    "#newmail.Display()\n",
    "\n",
    "newmail.Send()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM + Desktop automation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyautogui\n",
    "import pandas as pd\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.prompts import ChatPromptTemplate # Prompt templates convert raw user input to better input to the LLM (guide reponse)\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_core.output_parsers import StrOutputParser # original model output is a message but this function parses it to a string (easier to work with)\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain_community.embeddings import OllamaEmbeddings # converts documents into vector chunks\n",
    "from langchain_community.vectorstores import FAISS # simple local vector databse (vectorstore) in place of pinecone/chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain #This chain will take an incoming question, look up relevant documents, then pass those documents along with the original question into an LLM and \n",
    "from langchain.chains import create_retrieval_chain, create_history_aware_retriever                     #ask it to answer the original question.\n",
    "from langchain_community.vectorstores import Chroma # Vector database we'll use \n",
    "from langchain_core.messages import HumanMessage, AIMessage # Used to frame history to LLM and retriever\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"llama3\" # mistral, llama2, kdl_copilot_llama3, llama3, llama2:13b\n",
    "llm = Ollama(model=model)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "output1 = chain.invoke({\"input\": \"Generate me a list of 4 interesting questions to ask a librarian, where each question is separated by a comma\"}) # now returns a string rather than a ChatMessage which would normal include \\n for spacing\n",
    "output2 = chain.invoke({\"input\": f\"{output1} considering the output answer each question\"}) # now returns a string rather than a ChatMessage which would normal include \\n for spacing\n",
    "\n",
    "print(output2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pyautogui\u001b[38;5;241m.\u001b[39mhold(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwin\u001b[39m\u001b[38;5;124m'\u001b[39m): \n\u001b[0;32m      8\u001b[0m         pyautogui\u001b[38;5;241m.\u001b[39mpress([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mup\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 10\u001b[0m \u001b[43mpyautogui\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.00009\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pyautogui\u001b[38;5;241m.\u001b[39mhold(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mctrl\u001b[39m\u001b[38;5;124m'\u001b[39m): \n\u001b[0;32m     13\u001b[0m         pyautogui\u001b[38;5;241m.\u001b[39mpress([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pyautogui\\__init__.py:594\u001b[0m, in \u001b[0;36m_genericPyAutoGUIChecks.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(wrappedFunction)\n\u001b[0;32m    592\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    593\u001b[0m     failSafeCheck()\n\u001b[1;32m--> 594\u001b[0m     returnVal \u001b[38;5;241m=\u001b[39m wrappedFunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    595\u001b[0m     _handlePause(kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_pause\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m returnVal\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pyautogui\\__init__.py:1686\u001b[0m, in \u001b[0;36mtypewrite\u001b[1;34m(message, interval, logScreenshot, _pause)\u001b[0m\n\u001b[0;32m   1684\u001b[0m     c \u001b[38;5;241m=\u001b[39m c\u001b[38;5;241m.\u001b[39mlower()\n\u001b[0;32m   1685\u001b[0m press(c, _pause\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m-> 1686\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43minterval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1687\u001b[0m failSafeCheck()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pyautogui.PAUSE = .25\n",
    "\n",
    "pyautogui.press('win')  # press the windows key\n",
    "pyautogui.write('notepad', interval=0.25)\n",
    "pyautogui.press('enter')\n",
    "\n",
    "with pyautogui.hold('win'): \n",
    "        pyautogui.press(['up'])\n",
    "\n",
    "pyautogui.write(output2, interval=0.00009)\n",
    "\n",
    "with pyautogui.hold('ctrl'): \n",
    "        pyautogui.press(['s'])\n",
    "\n",
    "num = random.randint(0, 1000)\n",
    "\n",
    "pyautogui.write(f\"creative title {num}\", interval=0.2)\n",
    "pyautogui.press('enter')\n",
    "\n",
    "with pyautogui.hold('alt'): \n",
    "        pyautogui.press(['f4']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to add a fail safe way to exit the code\n",
    "\n",
    "import pyautogui\n",
    "import keyboard\n",
    "import randomer \n",
    "import time\n",
    "\n",
    "while True:\n",
    "    if keyboard.is_pressed('esc'):  # if key 'esc' is pressed \n",
    "        print('\\nExiting...')\n",
    "        break  # finishing the loop\n",
    "\n",
    "    pyautogui.press('win')  # press the windows key\n",
    "    pyautogui.write('notepad', interval=0.25)\n",
    "    pyautogui.press('enter')\n",
    "\n",
    "    with pyautogui.hold('win'): \n",
    "        pyautogui.press(['up'])\n",
    "\n",
    "    pyautogui.write(output1, interval=0.009)\n",
    "\n",
    "    with pyautogui.hold('ctrl'): \n",
    "        pyautogui.press(['s']) \n",
    "\n",
    "    num = random.randint(0, 1)  # adjust the range as needed\n",
    "\n",
    "    pyautogui.write(f\"creative title {num}\", interval=0.2)\n",
    "    pyautogui.press('enter')\n",
    "\n",
    "    time.sleep(2)  # add a delay if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 0 \n",
    "\n",
    "for coordinate in coordinates:\n",
    "    x, y = coordinate\n",
    "\n",
    "    pyautogui.moveTo(x, y)\n",
    "    pyautogui.click()\n",
    "    time.sleep(45)\n",
    "\n",
    "    with pyautogui.hold('alt'): \n",
    "        pyautogui.press(['f4'])\n",
    "\n",
    "    num += 1\n",
    "    print(f\"game {num} fully clicked\")\n",
    "\n",
    "    time.sleep(20)\n",
    "\n",
    "    pyautogui.moveTo(198, 296)\n",
    "    pyautogui.click()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
